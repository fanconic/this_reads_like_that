{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a686c638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import AG_NEWS, IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f2fda2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = AG_NEWS(split=\"train\")\n",
    "test_ds = AG_NEWS(split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eeabf81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "826508c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=1)\n",
    "test_loader = DataLoader(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "95f5e434",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader\n",
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "57f7f3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_pipeline(x): return int(x) - 1\n",
    "def collate_batch(batch):\n",
    "    \"\"\" Collate function to generate batches from the dataset\n",
    "    Args:\n",
    "        batch: current batch from the iterator\n",
    "    returns:\n",
    "        labels, text, offset\n",
    "    \"\"\"\n",
    "#     print(batch)\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(\n",
    "            tokenizer.encode(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "#         print(text_list)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "dataloader = DataLoader(train_ds, batch_size=1,\n",
    "                        shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4247566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3]), (\"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\",)]\n"
     ]
    }
   ],
   "source": [
    "for bla in train_loader:\n",
    "    print(bla)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7ea20816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer, GPT2LMHeadModel, GPT2ForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments, AutoModelWithLMHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2f765d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d510ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3]), (\"Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\",)]\n",
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "for bla in test_loader:\n",
    "    print(bla)\n",
    "    print(tokenizer.encode(bla[1]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8e009a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model2 = GPT2ForSequenceClassification.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "48931d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1aa37cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bla: (tensor([3, 3]), tensor([ 5497,   713,   902,  8554,  7446,   319, 15648,  8741,   453,   357,\n",
      "         2969,     8,  3486,   532, 30756,  1752,   550,   645,  3572,   475,\n",
      "          284,  4268,  7262,  2663,   611,   484,  6304,   470,  1498,   284,\n",
      "         4929,   257,  4099,   878,   262, 14195,   286, 11247, 21350,    13,\n",
      "          887, 11947,  1973,   262,  1499,  6481,   389,  7067,  2405,   640,\n",
      "           11,  5291,  4692,  2663,  6776,   416, 16056,   278, 27022, 38466,\n",
      "         1262,   511,  7446, 16545,    13,    50, 14474,  8545,  1001,   274,\n",
      "        20463,   263, 33137,   399,  3558,   357,  2969,     8,  3486,   532,\n",
      "         7994,  2063,   262,  6678,  1271,   286, 49706,  2256, 36288,   423,\n",
      "        28376,  1022,  2258,  5913,   290,  4744,   428,  1622,    11,   290,\n",
      "         5519,   423,   645,  7468,   329,   262,  4268,    13]), tensor([ 0, 65]))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-1a246f8f4b10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbla\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"bla: {bla}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenize: {tokenizer.tokenize(bla[1][0])}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"encode: {tokenizer.encode(bla[1][0])}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"model: {model(tokenizer.encode(bla[1][0]))}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0mno_split_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens_trie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;31m# [\"This is something\", \"<special_token_1>\", \"  else\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# Main loop, Giving this algorithm O(n) complexity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_char\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mskip\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Prevents the lookahead for matching twice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;31m# See gh-54457\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iteration over a 0-d tensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m             warnings.warn('Iterating over a tensor might cause the trace to be incorrect. '\n",
      "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d tensor"
     ]
    }
   ],
   "source": [
    "for bla in dataloader:\n",
    "    print(f\"bla: {bla}\")\n",
    "    print(f\"tokenize: {tokenizer.tokenize(bla[1][0])}\")\n",
    "    print(f\"encode: {tokenizer.encode(bla[1][0])}\")\n",
    "    print(f\"model: {model(tokenizer.encode(bla[1][0]))}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "61aee75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3]) tensor([ 2348, 17942,  2888,   969, 10920,   364,  3467,  7879,  7112,    56,\n",
      "         2885,    39,    11,  7420,  9671,   357, 18474,     8,  1377,  1881,\n",
      "          286,  7420,  9671,   338,   749,  2227, 13162,   468,    59, 33886,\n",
      "         2241,   656,   262,  4773,    11,   262,   717,  4664,  4099,   284,\n",
      "        16908,   739,    59,    64,   530,    12,  8424,  1230, 33652,  3414,\n",
      "          938,  1285,   526,  6852,     1,    46,   400,   805,   978,    12,\n",
      "           46,    76,  2743,    11,  1271,   678,   319,  7420,  9671,   338,\n",
      "          749,  2227,  1351,   286,  2608,    11,  6292,    59, 15708, 27361,\n",
      "           67,   338,  2897,   286, 33652,    11,   543,   373,   925,   938,\n",
      "         1285,    11,  1864,   284,  7420,    59,    82,  2203,  3321,   526,\n",
      "         6852,     1,  2348,    12,    46,    76,  2743,    11,   508,  2900,\n",
      "         2241,   287,   319,  3502,  1755,    11,   373,   257,  1597,  5212,\n",
      "          286,    59,  2484, 45094,   978,   911,  4449,   380,  1377,   262,\n",
      "          717,   435, 17942,  2888,   284,  2453,   262,  2897,   618,   339,\n",
      "         2900,    59, 38400,   944,   287,  3217,   526,  6852,  2215,   314,\n",
      "          717,  2497,   428,   314,   373,  1107,  9247,    13,   220,   314,\n",
      "         1807,   326,   340,   561,  3729,    59, 28230,   284,   517,  3685,\n",
      "          611,   484,   655,  1309, 10509,   572,   262,  2644,  6852])\n",
      "hello\n",
      "torch.Size([179, 4])\n",
      "goodbye\n",
      "output: (torch.Size([179, 50257]), None)\n"
     ]
    }
   ],
   "source": [
    "for (label, text, offsets) in dataloader:\n",
    "    print(label, text)\n",
    "#     print(f\"bla: {bla}\")\n",
    "#     print(f\"tokenize: {tokenizer.tokenize(bla[1][0])}\")\n",
    "#     print(f\"encode: {tokenizer.encode(bla[1][0])}\")\n",
    "    print(\"hello\")\n",
    "    output = model(text)\n",
    "    fc = nn.Linear(50257, 4)\n",
    "    output2 = fc(output.logits)\n",
    "    print(output2.shape)\n",
    "    \n",
    "    print('goodbye')\n",
    "    print(f\"output: {output.logits.shape, output.loss}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "187d3a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([4]), ('Ashlee Vance: the readers have spoken &lt;strong&gt;Poll results&lt;/strong&gt; Bright news for resident &lt;em&gt;Reg&lt;/em&gt; ladyboy',)]\n",
      "{'input_ids': tensor([[50256]]), 'attention_mask': tensor([[1]])}\n",
      "hello\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-186-29d9338c16ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     print(label, text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hello\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'goodbye'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#     print(output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1367\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1370\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You cannot specify both input_ids and inputs_embeds at the same time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m             \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "for bla in train_loader:\n",
    "    print(bla)\n",
    "    print(tokenizer.encode_plus(bla[1], return_tensors='pt'))\n",
    "    text = tokenizer.encode_plus(bla[1])\n",
    "#     print(label, text)\n",
    "    print(\"hello\")\n",
    "    output = model2(text['input_ids'])\n",
    "    print('goodbye')\n",
    "#     print(output)\n",
    "#     loss, logits = output.loss, output.logits\n",
    "    print(f\"output: {output.logits.shape, output.loss}\")\n",
    "#     print(f\"model: {model(text)}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d173fe49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70113f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4be985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ec6415ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([94, 50257])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "103bff4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T_destination',\n",
      " '__annotations__',\n",
      " '__call__',\n",
      " '__class__',\n",
      " '__delattr__',\n",
      " '__dict__',\n",
      " '__dir__',\n",
      " '__doc__',\n",
      " '__eq__',\n",
      " '__format__',\n",
      " '__ge__',\n",
      " '__getattr__',\n",
      " '__getattribute__',\n",
      " '__gt__',\n",
      " '__hash__',\n",
      " '__init__',\n",
      " '__init_subclass__',\n",
      " '__le__',\n",
      " '__lt__',\n",
      " '__module__',\n",
      " '__ne__',\n",
      " '__new__',\n",
      " '__reduce__',\n",
      " '__reduce_ex__',\n",
      " '__repr__',\n",
      " '__setattr__',\n",
      " '__setstate__',\n",
      " '__sizeof__',\n",
      " '__str__',\n",
      " '__subclasshook__',\n",
      " '__weakref__',\n",
      " '_apply',\n",
      " '_call_impl',\n",
      " '_convert_head_mask_to_5d',\n",
      " '_create_or_get_repo',\n",
      " '_expand_inputs_for_generation',\n",
      " '_from_config',\n",
      " '_get_backward_hooks',\n",
      " '_get_decoder_start_token_id',\n",
      " '_get_logits_processor',\n",
      " '_get_logits_warper',\n",
      " '_get_name',\n",
      " '_get_pad_token_id',\n",
      " '_get_repo_url_from_name',\n",
      " '_get_resized_embeddings',\n",
      " '_get_resized_lm_head',\n",
      " '_get_stopping_criteria',\n",
      " '_hook_rss_memory_post_forward',\n",
      " '_hook_rss_memory_pre_forward',\n",
      " '_init_weights',\n",
      " '_keys_to_ignore_on_load_missing',\n",
      " '_keys_to_ignore_on_load_unexpected',\n",
      " '_keys_to_ignore_on_save',\n",
      " '_load_from_state_dict',\n",
      " '_load_state_dict_into_model',\n",
      " '_load_state_dict_into_model_low_mem',\n",
      " '_maybe_warn_non_full_backward_hook',\n",
      " '_named_members',\n",
      " '_prepare_attention_mask_for_generation',\n",
      " '_prepare_decoder_input_ids_for_generation',\n",
      " '_prepare_encoder_decoder_kwargs_for_generation',\n",
      " '_prepare_input_ids_for_generation',\n",
      " '_push_to_hub',\n",
      " '_register_load_state_dict_pre_hook',\n",
      " '_register_state_dict_hook',\n",
      " '_reorder_cache',\n",
      " '_replicate_for_data_parallel',\n",
      " '_resize_token_embeddings',\n",
      " '_save_to_state_dict',\n",
      " '_set_default_torch_dtype',\n",
      " '_set_gradient_checkpointing',\n",
      " '_slow_forward',\n",
      " '_tie_encoder_decoder_weights',\n",
      " '_tie_or_clone_weights',\n",
      " '_update_model_kwargs_for_generation',\n",
      " '_version',\n",
      " 'add_memory_hooks',\n",
      " 'add_module',\n",
      " 'adjust_logits_during_generation',\n",
      " 'apply',\n",
      " 'base_model',\n",
      " 'base_model_prefix',\n",
      " 'beam_sample',\n",
      " 'beam_search',\n",
      " 'bfloat16',\n",
      " 'buffers',\n",
      " 'children',\n",
      " 'config_class',\n",
      " 'cpu',\n",
      " 'cuda',\n",
      " 'deparallelize',\n",
      " 'device',\n",
      " 'double',\n",
      " 'dtype',\n",
      " 'dummy_inputs',\n",
      " 'dump_patches',\n",
      " 'estimate_tokens',\n",
      " 'eval',\n",
      " 'extra_repr',\n",
      " 'float',\n",
      " 'floating_point_ops',\n",
      " 'forward',\n",
      " 'framework',\n",
      " 'from_pretrained',\n",
      " 'generate',\n",
      " 'get_buffer',\n",
      " 'get_extended_attention_mask',\n",
      " 'get_extra_state',\n",
      " 'get_head_mask',\n",
      " 'get_input_embeddings',\n",
      " 'get_output_embeddings',\n",
      " 'get_parameter',\n",
      " 'get_position_embeddings',\n",
      " 'get_submodule',\n",
      " 'gradient_checkpointing_disable',\n",
      " 'gradient_checkpointing_enable',\n",
      " 'greedy_search',\n",
      " 'group_beam_search',\n",
      " 'half',\n",
      " 'init_weights',\n",
      " 'invert_attention_mask',\n",
      " 'is_gradient_checkpointing',\n",
      " 'is_parallelizable',\n",
      " 'load_state_dict',\n",
      " 'load_tf_weights',\n",
      " 'modules',\n",
      " 'named_buffers',\n",
      " 'named_children',\n",
      " 'named_modules',\n",
      " 'named_parameters',\n",
      " 'num_parameters',\n",
      " 'parallelize',\n",
      " 'parameters',\n",
      " 'prepare_inputs_for_generation',\n",
      " 'prune_heads',\n",
      " 'push_to_hub',\n",
      " 'register_backward_hook',\n",
      " 'register_buffer',\n",
      " 'register_forward_hook',\n",
      " 'register_forward_pre_hook',\n",
      " 'register_full_backward_hook',\n",
      " 'register_parameter',\n",
      " 'requires_grad_',\n",
      " 'reset_memory_hooks_state',\n",
      " 'resize_position_embeddings',\n",
      " 'resize_token_embeddings',\n",
      " 'retrieve_modules_from_names',\n",
      " 'sample',\n",
      " 'save_pretrained',\n",
      " 'set_extra_state',\n",
      " 'set_input_embeddings',\n",
      " 'set_output_embeddings',\n",
      " 'share_memory',\n",
      " 'state_dict',\n",
      " 'supports_gradient_checkpointing',\n",
      " 'tie_weights',\n",
      " 'to',\n",
      " 'to_empty',\n",
      " 'train',\n",
      " 'type',\n",
      " 'xpu',\n",
      " 'zero_grad']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(dir(GPT2ForSequenceClassification))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "090a9e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'>\n"
     ]
    }
   ],
   "source": [
    "print(GPT2LMHeadModel.config_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fd9550e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-856230469917>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT_co\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__add__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Dataset[T_co]'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'ConcatDataset[T_co]'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "2df955df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0911b50efe3744c4ae0e81f11b8deba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a978f39935647bf94388b48487d08b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9cdb7be69d4a21aff69113072584df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b108d86ea7b24811a3d80840adffff25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c07380beba46c4be01618ce05e00fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/413M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should be paraphrase\n",
      "not paraphrase: 10%\n",
      "is paraphrase: 90%\n",
      "\n",
      "Should not be paraphrase\n",
      "not paraphrase: 94%\n",
      "is paraphrase: 6%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "\n",
    "paraphrase = tokenizer.encode_plus(sequence_0, sequence_2, return_tensors=\"pt\")\n",
    "not_paraphrase = tokenizer.encode_plus(sequence_0, sequence_1, return_tensors=\"pt\")\n",
    "\n",
    "paraphrase_classification_logits = model(**paraphrase)[0]\n",
    "not_paraphrase_classification_logits = model(**not_paraphrase)[0]\n",
    "\n",
    "paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "\n",
    "print(\"Should be paraphrase\")\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {round(paraphrase_results[i] * 100)}%\")\n",
    "\n",
    "print(\"\\nShould not be paraphrase\")\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {round(not_paraphrase_results[i] * 100)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "0162900a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3456, -3.7659]], grad_fn=<IndexBackward0>)\n",
      "paraphrase: {'input_ids': tensor([[  464,  1664, 12905,  2667, 32388,   318,  1912,   287,   968,  1971,\n",
      "          2254]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Should be paraphrase\n",
      "not paraphrase: 98%\n",
      "is paraphrase: 2%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\")\n",
    "\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "\n",
    "paraphrase = tokenizer.encode_plus(sequence_0, return_tensors=\"pt\")\n",
    "# not_paraphrase = tokenizer.encode_plus(sequence_0, sequence_1, return_tensors=\"pt\")\n",
    "\n",
    "paraphrase_classification_logits = model(**paraphrase)[0]\n",
    "print(paraphrase_classification_logits)\n",
    "print(f\"paraphrase: {paraphrase}\")\n",
    "# print(model(**paraphrase))\n",
    "# not_paraphrase_classification_logits = model(**not_paraphrase)[0]\n",
    "\n",
    "paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "# not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "\n",
    "print(\"Should be paraphrase\")\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {round(paraphrase_results[i] * 100)}%\")\n",
    "\n",
    "# print(\"\\nShould not be paraphrase\")\n",
    "# for i in range(len(classes)):\n",
    "#     print(f\"{classes[i]}: {round(not_paraphrase_results[i] * 100)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76e728f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
