name: "weighing_dims"
random_state: 42
wandb_logging: False

data:
    dataset: "reviews"
    val_size: 0.2
    data_name: "rt-polarity"
    data_dir: "./src/data"
    compute_emb: False

train:
    batch_size: 128
    epochs: 100
    verbose: True

model:
    name: "proto"
    submodel: "bert" #if not a protonet, called it same as "name"
    embed_dim: 1024 #1024 for sentence + bert
    n_classes: 2
    freeze_layers: True
    n_prototypes: 20 #Make sure that n_prototypes/n_classes gives integer
    similaritymeasure: "weighted cosine" #L2 or cosine or weighted cosine
    embedding: "sentence" #sentence or word
    project: True

optimizer:
    name: "Adam" #SGD or Adam
    lr: 0.001
    momentum: 0.9
    weight_decay: 0.0005
    betas: [0.9, 0.999]
    T_0: 10

scheduler:
    name: "step"
    lr_reduce_factor: 0.5
    patience_lr_reduce: 30
    poly_reduce: 0.9

loss:
    lambda1: 0.2
    lambda2: 0.2
    lambda3: 0.3
    lambda4: 0.1
    lambda5: 0.001
